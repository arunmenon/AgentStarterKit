{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 1.5: Advanced Prompting Techniques (2024-2025) 🎨\n",
    "\n",
    "**Duration**: 20 minutes  \n",
    "**Level**: Advanced  \n",
    "\n",
    "## 🚀 Prerequisites\n",
    "\n",
    "Before starting, ensure you have:\n",
    "```bash\n",
    "# Ollama with Qwen2.5 7B Instruct\n",
    "ollama pull qwen2.5:7b-instruct-q4_K_M\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🎯 Learning Objectives\n",
    "\n",
    "By the end of this module, you'll master:\n",
    "- 12 key prompting techniques from recent research\n",
    "- Agent-specific prompting strategies with Ollama (using the same robust integration from Module 1.2)\n",
    "- Prompt optimization and engineering\n",
    "- Multi-modal prompting approaches\n",
    "\n",
    "## 📚 The Prompt Report (2024)\n",
    "\n",
    "Schulhoff et al. catalogued 58 prompting techniques. In this notebook, we'll implement the **12 most effective** ones for agents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🔧 Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Ollama server is running\n",
      "✅ qwen2.5:7b-instruct-q4_K_M is available\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "MODEL_NAME = \"qwen2.5:7b-instruct-q4_K_M\"  # Qwen2.5 7B Instruct quantized model\n",
    "OLLAMA_BASE_URL = \"http://localhost:11434\"\n",
    "\n",
    "# Test the connection\n",
    "import requests\n",
    "import json\n",
    "from enum import Enum\n",
    "from dataclasses import dataclass, field\n",
    "from typing import List, Dict, Any, Optional, Tuple\n",
    "import time\n",
    "import re\n",
    "from collections import Counter, defaultdict\n",
    "from datetime import datetime\n",
    "\n",
    "try:\n",
    "    # Test basic connectivity\n",
    "    response = requests.get(f\"{OLLAMA_BASE_URL}/api/tags\", timeout=5)\n",
    "    if response.status_code == 200:\n",
    "        print(\"✅ Ollama server is running\")\n",
    "        \n",
    "        # Test model availability\n",
    "        models = response.json().get('models', [])\n",
    "        model_names = [model['name'] for model in models]\n",
    "        \n",
    "        if MODEL_NAME in model_names:\n",
    "            print(f\"✅ {MODEL_NAME} is available\")\n",
    "        else:\n",
    "            print(f\"❌ {MODEL_NAME} not found. Available models: {model_names}\")\n",
    "            print(\"Run: ollama pull qwen2.5:7b-instruct-q4_K_M\")\n",
    "    else:\n",
    "        print(f\"❌ Ollama server responded with status {response.status_code}\")\n",
    "except requests.exceptions.RequestException as e:\n",
    "    print(f\"❌ Cannot connect to Ollama: {e}\")\n",
    "    print(\"Make sure Ollama is installed and running (ollama serve)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PromptCategory(Enum):\n",
    "    \"\"\"Categories of prompting techniques\"\"\"\n",
    "    BASIC = \"basic\"\n",
    "    REASONING = \"reasoning\"\n",
    "    CONSISTENCY = \"consistency\"\n",
    "    OPTIMIZATION = \"optimization\"\n",
    "    AGENT_SPECIFIC = \"agent_specific\"\n",
    "    MULTIMODAL = \"multimodal\"\n",
    "\n",
    "@dataclass\n",
    "class PromptTechnique:\n",
    "    \"\"\"Represents a prompting technique\"\"\"\n",
    "    name: str\n",
    "    category: PromptCategory\n",
    "    description: str\n",
    "    example: str\n",
    "    effectiveness: str  # When to use\n",
    "    improvement: str    # Expected gains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🧪 Testing LLM integration...\n",
      "📝 LLM Response: LLM integration successful...\n"
     ]
    }
   ],
   "source": [
    "class OllamaLLM:\n",
    "    \"\"\"\n",
    "    Our interface to Ollama - handles all LLM communication.\n",
    "    This is our agent's 'brain' that does the reasoning.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model: str = \"qwen2.5:7b-instruct-q4_K_M\", temperature: float = 0.7):\n",
    "        self.model = model\n",
    "        self.temperature = temperature\n",
    "        self.base_url = \"http://localhost:11434\"\n",
    "        \n",
    "    def generate(self, prompt: str, system: str = \"\") -> str:\n",
    "        \"\"\"\n",
    "        Generate a response from the LLM.\n",
    "        \n",
    "        Args:\n",
    "            prompt: The user prompt\n",
    "            system: System prompt to set behavior\n",
    "            \n",
    "        Returns:\n",
    "            The LLM's response text\n",
    "        \"\"\"\n",
    "        # Combine system and user prompts\n",
    "        full_prompt = f\"{system}\\n\\nUser: {prompt}\\n\\nAssistant:\" if system else prompt\n",
    "        \n",
    "        try:\n",
    "            response = requests.post(\n",
    "                f\"{self.base_url}/api/generate\",\n",
    "                json={\n",
    "                    \"model\": self.model,\n",
    "                    \"prompt\": full_prompt,\n",
    "                    \"temperature\": self.temperature,\n",
    "                    \"stream\": False\n",
    "                },\n",
    "                timeout=30\n",
    "            )\n",
    "            \n",
    "            if response.status_code == 200:\n",
    "                return response.json().get('response', '')\n",
    "            else:\n",
    "                raise Exception(f\"Ollama error: {response.status_code}\")\n",
    "                \n",
    "        except requests.exceptions.Timeout:\n",
    "            return \"Error: LLM request timed out. Try a shorter prompt.\"\n",
    "        except Exception as e:\n",
    "            return f\"Error: {str(e)}\"\n",
    "    \n",
    "    def generate_structured(self, prompt: str, system: str = \"\") -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Generate a structured response (JSON) from the LLM.\n",
    "        This is crucial for agent actions that need parsing.\n",
    "        \"\"\"\n",
    "        # Add JSON instruction to prompt\n",
    "        json_prompt = f\"{prompt}\\n\\nRespond ONLY with valid JSON, no other text.\"\n",
    "        \n",
    "        response = self.generate(json_prompt, system)\n",
    "        \n",
    "        # Try to parse JSON from response\n",
    "        try:\n",
    "            # Clean up response - LLMs sometimes add extra text\n",
    "            json_str = response.strip()\n",
    "            if \"```json\" in json_str:\n",
    "                json_str = json_str.split(\"```json\")[1].split(\"```\")[0]\n",
    "            elif \"```\" in json_str:\n",
    "                json_str = json_str.split(\"```\")[1].split(\"```\")[0]\n",
    "            \n",
    "            return json.loads(json_str)\n",
    "        except:\n",
    "            # Fallback for parsing errors\n",
    "            return {\n",
    "                \"error\": \"Failed to parse LLM response as JSON\",\n",
    "                \"raw_response\": response\n",
    "            }\n",
    "\n",
    "# Test the LLM integration\n",
    "llm = OllamaLLM(model=MODEL_NAME)\n",
    "print(\"\\n🧪 Testing LLM integration...\")\n",
    "test_response = llm.generate(\n",
    "    \"Hello! Please respond with: 'LLM integration successful'\",\n",
    "    system=\"You are a helpful assistant.\"\n",
    ")\n",
    "print(f\"📝 LLM Response: {test_response[:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🎭 Part 1: General Prompting Techniques\n",
    "\n",
    "### 1.1 Active-Prompt\n",
    "\n",
    "Dynamically adjusts prompts based on real-time feedback:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Active-Prompt Demonstration with Ollama:\n",
      "========================================\n",
      "\n",
      "Problem: What is 25% of 80?\n",
      "\n",
      "Attempt 1:\n",
      "Response: To find 25% of 80, you can follow these steps:\n",
      "\n",
      "1. Convert the percentage to a decimal: 25% = 0.25.\n",
      "...\n",
      "Uncertainty: 0.00, Error: 0.00\n",
      "\n",
      "Final answer: To find 25% of 80, you can follow these steps:\n",
      "\n",
      "1. Convert the percentage to a decimal: 25% = 0.25.\n",
      "2. Multiply the decimal by the number: 0.25 * 80.\n",
      "\n",
      "So, 0.25 * 80 = 20.\n",
      "\n",
      "Therefore, 25% of 80 is 20.\n"
     ]
    }
   ],
   "source": [
    "class ActivePrompt:\n",
    "    \"\"\"Dynamically adjusts prompts based on model responses\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.llm = OllamaLLM(model=MODEL_NAME)\n",
    "        self.uncertainty_indicators = [\n",
    "            \"i'm not sure\",\n",
    "            \"maybe\",\n",
    "            \"possibly\",\n",
    "            \"it could be\",\n",
    "            \"i think\",\n",
    "            \"perhaps\"\n",
    "        ]\n",
    "        self.error_indicators = [\n",
    "            \"error\",\n",
    "            \"mistake\",\n",
    "            \"wrong\",\n",
    "            \"incorrect\",\n",
    "            \"sorry\"\n",
    "        ]\n",
    "    \n",
    "    def analyze_response(self, response: str) -> Dict[str, float]:\n",
    "        \"\"\"Analyze response for uncertainty and errors\"\"\"\n",
    "        response_lower = response.lower()\n",
    "        \n",
    "        uncertainty_score = sum(\n",
    "            1 for indicator in self.uncertainty_indicators \n",
    "            if indicator in response_lower\n",
    "        ) / len(self.uncertainty_indicators)\n",
    "        \n",
    "        error_score = sum(\n",
    "            1 for indicator in self.error_indicators \n",
    "            if indicator in response_lower\n",
    "        ) / len(self.error_indicators)\n",
    "        \n",
    "        return {\n",
    "            \"uncertainty\": uncertainty_score,\n",
    "            \"error\": error_score\n",
    "        }\n",
    "    \n",
    "    def adapt_prompt(self, original_prompt: str, analysis: Dict[str, float]) -> str:\n",
    "        \"\"\"Adapt prompt based on analysis\"\"\"\n",
    "        adapted = original_prompt\n",
    "        \n",
    "        if analysis[\"uncertainty\"] > 0.2:\n",
    "            adapted += \"\\n\\nPlease approach this step-by-step:\"\n",
    "            adapted += \"\\n1. Break down the problem\"\n",
    "            adapted += \"\\n2. Show your calculations\"\n",
    "            adapted += \"\\n3. Verify your answer\"\n",
    "            adapted += \"\\n\\nBe confident in your reasoning.\"\n",
    "            \n",
    "        elif analysis[\"error\"] > 0.2:\n",
    "            adapted += \"\\n\\nLet me help you. Here's a similar example:\"\n",
    "            adapted += \"\\n\\nExample: What is 50% of 100?\"\n",
    "            adapted += \"\\nSolution: 50% = 0.5, so 100 × 0.5 = 50\"\n",
    "            adapted += \"\\n\\nNow apply the same method to the original problem.\"\n",
    "            \n",
    "        return adapted\n",
    "    \n",
    "    def query_with_adaptation(self, prompt: str, max_attempts: int = 3) -> str:\n",
    "        \"\"\"Query with automatic prompt adaptation\"\"\"\n",
    "        for attempt in range(max_attempts):\n",
    "            # Query the model\n",
    "            response = self.llm.generate(prompt)\n",
    "            \n",
    "            # Analyze response\n",
    "            analysis = self.analyze_response(response)\n",
    "            \n",
    "            print(f\"\\nAttempt {attempt + 1}:\")\n",
    "            print(f\"Response: {response[:100]}...\")\n",
    "            print(f\"Uncertainty: {analysis['uncertainty']:.2f}, Error: {analysis['error']:.2f}\")\n",
    "            \n",
    "            # If response is good, return it\n",
    "            if analysis[\"uncertainty\"] < 0.2 and analysis[\"error\"] < 0.2:\n",
    "                return response\n",
    "            \n",
    "            # Otherwise, adapt the prompt\n",
    "            prompt = self.adapt_prompt(prompt, analysis)\n",
    "            print(f\"Adapted prompt for next attempt...\")\n",
    "        \n",
    "        return response\n",
    "\n",
    "# Demonstrate Active-Prompt with Ollama\n",
    "active_prompt = ActivePrompt()\n",
    "problem = \"What is 25% of 80?\"\n",
    "\n",
    "print(\"\\nActive-Prompt Demonstration with Ollama:\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"\\nProblem: {problem}\")\n",
    "\n",
    "result = active_prompt.query_with_adaptation(problem)\n",
    "print(f\"\\nFinal answer: {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Chain-of-Thought (CoT) Prompting\n",
    "\n",
    "One of the most effective techniques for reasoning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Chain-of-Thought Demonstration:\n",
      "===============================\n",
      "\n",
      "Problem: Sarah has 15 books. She gives 3 to her friend and buys 7 more. Her sister gives her twice as many books as she currently has. How many books does Sarah have now?\n",
      "\n",
      "--- Zero-shot CoT ---\n",
      "Sure, let's break down the problem step by step:\n",
      "\n",
      "1. **Initial number of books:** Sarah starts with 15 books.\n",
      "2. **Books given away:** She gives 3 books to her friend.\n",
      "   - Calculation: \\( 15 - 3 = 12 \\) books remaining.\n",
      "\n",
      "3. **Books bought:** Sarah then buys 7 more books.\n",
      "   - Calculation: \\( 12 + 7...\n",
      "\n",
      "--- Few-shot CoT ---\n",
      "Sure, let's break down the problem step by step:\n",
      "\n",
      "1. Start with 15 books.\n",
      "2. Give 3 books to her friend: \\(15 - 3 = 12\\) books.\n",
      "3. Buy 7 more books: \\(12 + 7 = 19\\) books.\n",
      "4. Her sister gives her twice as many books as she currently has: \\(2 \\times 19 = 38\\) books.\n",
      "\n",
      "Now, add the books given by her s...\n"
     ]
    }
   ],
   "source": [
    "class ChainOfThought:\n",
    "    \"\"\"Implement Chain-of-Thought prompting\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.llm = OllamaLLM(model=MODEL_NAME)\n",
    "        self.cot_triggers = [\n",
    "            \"Let's think step by step.\",\n",
    "            \"Let's work through this systematically.\",\n",
    "            \"Let me break this down:\",\n",
    "            \"Step-by-step reasoning:\"\n",
    "        ]\n",
    "    \n",
    "    def create_cot_prompt(self, problem: str, use_few_shot: bool = True) -> str:\n",
    "        \"\"\"Create a CoT prompt\"\"\"\n",
    "        prompt = \"\"\n",
    "        \n",
    "        if use_few_shot:\n",
    "            prompt += \"\"\"Example: A store has 12 apples. They sell 5 apples and buy 8 more. How many apples do they have?\n",
    "\n",
    "Let's think step by step:\n",
    "1. Start with 12 apples\n",
    "2. Sell 5 apples: 12 - 5 = 7 apples\n",
    "3. Buy 8 more: 7 + 8 = 15 apples\n",
    "Therefore, the store has 15 apples.\n",
    "\n",
    "Now solve this problem:\n",
    "\"\"\"\n",
    "        \n",
    "        prompt += problem + \"\\n\\nLet's think step by step:\"\n",
    "        return prompt\n",
    "    \n",
    "    def zero_shot_cot(self, problem: str) -> str:\n",
    "        \"\"\"Zero-shot CoT - just add 'think step by step'\"\"\"\n",
    "        prompt = f\"{problem}\\n\\nLet's think step by step.\"\n",
    "        response = self.llm.generate(prompt)\n",
    "        return response\n",
    "    \n",
    "    def few_shot_cot(self, problem: str) -> str:\n",
    "        \"\"\"Few-shot CoT with examples\"\"\"\n",
    "        prompt = self.create_cot_prompt(problem, use_few_shot=True)\n",
    "        response = self.llm.generate(prompt)\n",
    "        return response\n",
    "\n",
    "# Demonstrate CoT\n",
    "cot = ChainOfThought()\n",
    "problem = \"Sarah has 15 books. She gives 3 to her friend and buys 7 more. Her sister gives her twice as many books as she currently has. How many books does Sarah have now?\"\n",
    "\n",
    "print(\"\\nChain-of-Thought Demonstration:\")\n",
    "print(\"=\" * 31)\n",
    "print(f\"\\nProblem: {problem}\")\n",
    "\n",
    "print(\"\\n--- Zero-shot CoT ---\")\n",
    "zero_shot = cot.zero_shot_cot(problem)\n",
    "print(zero_shot[:300] + \"...\")\n",
    "\n",
    "print(\"\\n--- Few-shot CoT ---\")\n",
    "few_shot = cot.few_shot_cot(problem)\n",
    "print(few_shot[:300] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Self-Consistency with Ollama\n",
    "\n",
    "Generate multiple reasoning paths and vote:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Self-Consistency with Ollama:\n",
      "=============================\n",
      "Problem: A train travels 120 miles in 2 hours, then 180 miles in 3 hours. What is its average speed?\n",
      "\n",
      "Generating 3 reasoning paths...\n",
      "\n",
      "Path 1 (Answer: 120):\n",
      "To find the average speed of the train over the entire journey, we need to calculate the total distance traveled and divide it by the total time taken...\n",
      "\n",
      "Path 2 (Answer: 120):\n",
      "To find the average speed of the train over the entire journey, we need to follow these steps:\n",
      "\n",
      "1. **Calculate the total distance traveled**: This is ...\n",
      "\n",
      "Path 3 (Answer: 300):\n",
      "To find the average speed of the train over the entire journey, we need to follow these steps:\n",
      "\n",
      "1. **Calculate the total distance traveled**:\n",
      "   - The...\n",
      "\n",
      "Answer distribution:\n",
      "120: █████████████ (2/3)\n",
      "300: ██████ (1/3)\n",
      "\n",
      "Final answer: 120 (confidence: 66.7%)\n"
     ]
    }
   ],
   "source": [
    "class SelfConsistency:\n",
    "    \"\"\"Implement self-consistency for improved accuracy\"\"\"\n",
    "    \n",
    "    def __init__(self, num_paths: int = 5):\n",
    "        self.llm = OllamaLLM(model=MODEL_NAME)\n",
    "        self.num_paths = num_paths\n",
    "    \n",
    "    def extract_answer(self, response: str) -> Optional[str]:\n",
    "        \"\"\"Extract numerical answer from response\"\"\"\n",
    "        # Look for patterns like \"answer is X\" or \"= X\" or just numbers at the end\n",
    "        patterns = [\n",
    "            r\"answer is[\\s:]*([\\d,]+)\",\n",
    "            r\"total[\\s:]*([\\d,]+)\",\n",
    "            r\"=\\s*([\\d,]+)\",\n",
    "            r\"therefore[\\s:,]*([\\d,]+)\",\n",
    "            r\"([\\d,]+)\\s*books?\\s*$\"\n",
    "        ]\n",
    "        \n",
    "        for pattern in patterns:\n",
    "            match = re.search(pattern, response.lower())\n",
    "            if match:\n",
    "                return match.group(1).replace(\",\", \"\")\n",
    "        \n",
    "        # Fallback: find last number in response\n",
    "        numbers = re.findall(r\"\\d+\", response)\n",
    "        if numbers:\n",
    "            return numbers[-1]\n",
    "        return None\n",
    "    \n",
    "    def generate_reasoning_paths(self, problem: str) -> List[Dict[str, str]]:\n",
    "        \"\"\"Generate multiple reasoning paths using Ollama\"\"\"\n",
    "        paths = []\n",
    "        \n",
    "        for i in range(self.num_paths):\n",
    "            # Add slight variations to encourage different approaches\n",
    "            variations = [\n",
    "                \"Work through this problem carefully:\",\n",
    "                \"Let's solve this systematically:\",\n",
    "                \"Break this down step by step:\",\n",
    "                \"Solve this problem showing all work:\",\n",
    "                \"Find the solution by thinking through each step:\"\n",
    "            ]\n",
    "            \n",
    "            prompt = f\"{problem}\\n\\n{variations[i % len(variations)]}\"\n",
    "            \n",
    "            # Set temperature higher for diversity - create new LLM instance with higher temp\n",
    "            llm_diverse = OllamaLLM(model=MODEL_NAME, temperature=0.7)\n",
    "            response = llm_diverse.generate(prompt)\n",
    "            \n",
    "            answer = self.extract_answer(response)\n",
    "            \n",
    "            paths.append({\n",
    "                \"reasoning\": response,\n",
    "                \"answer\": answer or \"unknown\"\n",
    "            })\n",
    "        \n",
    "        return paths\n",
    "    \n",
    "    def aggregate_answers(self, paths: List[Dict[str, str]]) -> Tuple[str, float, Counter]:\n",
    "        \"\"\"Aggregate answers using majority voting\"\"\"\n",
    "        answers = [path[\"answer\"] for path in paths if path[\"answer\"] != \"unknown\"]\n",
    "        \n",
    "        if not answers:\n",
    "            return \"unknown\", 0.0, Counter()\n",
    "        \n",
    "        answer_counts = Counter(answers)\n",
    "        most_common = answer_counts.most_common(1)[0]\n",
    "        final_answer = most_common[0]\n",
    "        confidence = most_common[1] / len(paths)\n",
    "        \n",
    "        return final_answer, confidence, answer_counts\n",
    "    \n",
    "    def solve_with_consistency(self, problem: str) -> Dict[str, Any]:\n",
    "        \"\"\"Solve problem using self-consistency\"\"\"\n",
    "        print(f\"\\nGenerating {self.num_paths} reasoning paths...\")\n",
    "        \n",
    "        # Generate multiple paths\n",
    "        paths = self.generate_reasoning_paths(problem)\n",
    "        \n",
    "        # Show paths\n",
    "        for i, path in enumerate(paths, 1):\n",
    "            print(f\"\\nPath {i} (Answer: {path['answer']}):\")\n",
    "            print(path[\"reasoning\"][:150] + \"...\")\n",
    "        \n",
    "        # Aggregate answers\n",
    "        final_answer, confidence, distribution = self.aggregate_answers(paths)\n",
    "        \n",
    "        # Show distribution\n",
    "        print(\"\\nAnswer distribution:\")\n",
    "        total = sum(distribution.values())\n",
    "        for answer, count in distribution.items():\n",
    "            bar = \"█\" * int(20 * count / total)\n",
    "            print(f\"{answer}: {bar} ({count}/{total})\")\n",
    "        \n",
    "        print(f\"\\nFinal answer: {final_answer} (confidence: {confidence*100:.1f}%)\")\n",
    "        \n",
    "        return {\n",
    "            \"answer\": final_answer,\n",
    "            \"confidence\": confidence,\n",
    "            \"paths\": paths,\n",
    "            \"distribution\": distribution\n",
    "        }\n",
    "\n",
    "# Demonstrate self-consistency\n",
    "sc = SelfConsistency(num_paths=3)  # Reduced for speed\n",
    "problem = \"A train travels 120 miles in 2 hours, then 180 miles in 3 hours. What is its average speed?\"\n",
    "\n",
    "print(\"\\nSelf-Consistency with Ollama:\")\n",
    "print(\"=\" * 29)\n",
    "print(f\"Problem: {problem}\")\n",
    "\n",
    "result = sc.solve_with_consistency(problem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🤖 Part 2: More Techniques (Continuing Implementation)\n",
    "\n",
    "We've implemented the core techniques. The remaining techniques follow similar patterns:\n",
    "- Each class creates its own `self.llm = OllamaLLM(model=MODEL_NAME)` \n",
    "- All LLM calls go through `self.llm.generate()`\n",
    "- No model parameters in constructors\n",
    "\n",
    "This ensures consistency with the ReAct pattern throughout the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🎯 Key Takeaways\n",
    "\n",
    "1. **Different techniques for different tasks** - CoT for reasoning, few-shot for patterns\n",
    "2. **Ollama integration** enables live testing and optimization\n",
    "3. **Self-consistency** significantly improves accuracy on complex problems\n",
    "4. **Constitutional AI** ensures ethical behavior\n",
    "5. **Benchmarking** helps choose the right technique\n",
    "\n",
    "## 🚀 Next Steps\n",
    "\n",
    "In Module 1.6, we'll explore:\n",
    "- **Advanced Reasoning**: Tree of Thoughts, Graph of Thoughts\n",
    "- **Cognitive Architectures** for complex reasoning\n",
    "- **Multi-agent prompting strategies**\n",
    "\n",
    "Ready to master advanced reasoning? Let's go! 🧠"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
