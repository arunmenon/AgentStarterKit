{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 1.2: The ReAct Pattern - Complete Implementation ðŸ”„\n",
    "\n",
    "**Duration**: 30 minutes  \n",
    "**Level**: Foundation  \n",
    "\n",
    "## ðŸŽ¯ Learning Objectives\n",
    "\n",
    "By the end of this module, you'll understand:\n",
    "\n",
    "1. **Agent Architecture**: The four core components every agent needs\n",
    "2. **ReAct Pattern**: How agents think, act, and learn from observations  \n",
    "3. **State Management**: Tracking agent progress and context\n",
    "4. **Tool Integration**: Building and using external capabilities\n",
    "5. **Function Calling**: Structured communication with language models\n",
    "\n",
    "## ðŸš€ Prerequisites\n",
    "\n",
    "Before starting, ensure you have:\n",
    "```bash\n",
    "# Ollama with Qwen2.5 7B Instruct\n",
    "ollama pull qwen2.5:7b-instruct-q4_K_M\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ”§ Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "MODEL_NAME = \"qwen2.5:7b-instruct-q4_K_M\"  # Qwen2.5 7B Instruct quantized model\n",
    "OLLAMA_BASE_URL = \"http://localhost:11434\"\n",
    "\n",
    "# Test the connection\n",
    "import requests\n",
    "import json\n",
    "from enum import Enum\n",
    "from dataclasses import dataclass, field\n",
    "from typing import List, Dict, Any, Optional, Tuple\n",
    "import time\n",
    "\n",
    "try:\n",
    "    # Test basic connectivity\n",
    "    response = requests.get(f\"{OLLAMA_BASE_URL}/api/tags\", timeout=5)\n",
    "    if response.status_code == 200:\n",
    "        print(\"âœ… Ollama server is running\")\n",
    "        \n",
    "        # Test model availability\n",
    "        models = response.json().get('models', [])\n",
    "        model_names = [model['name'] for model in models]\n",
    "        \n",
    "        if MODEL_NAME in model_names:\n",
    "            print(f\"âœ… {MODEL_NAME} is available\")\n",
    "        else:\n",
    "            print(f\"âŒ {MODEL_NAME} not found. Available models: {model_names}\")\n",
    "            print(\"Run: ollama pull qwen2.5:7b-instruct-q4_K_M\")\n",
    "    else:\n",
    "        print(f\"âŒ Ollama server responded with status {response.status_code}\")\n",
    "except requests.exceptions.RequestException as e:\n",
    "    print(f\"âŒ Cannot connect to Ollama: {e}\")\n",
    "    print(\"Make sure Ollama is installed and running (ollama serve)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ—ï¸ Part 1: Core Agent Architecture\n",
    "\n",
    "We'll construct our agent using these fundamental components:\n",
    "\n",
    "1. **State** - The agent's memory and context\n",
    "2. **Brain** - LLM integration for reasoning\n",
    "3. **Tools** - Capabilities beyond text generation\n",
    "4. **Controller** - Orchestrates the agent loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's define agent states - this helps us track what the agent is doing\n",
    "class AgentState(Enum):\n",
    "    \"\"\"Possible states for our agent - like a state machine\"\"\"\n",
    "    IDLE = \"idle\"              # Waiting for a task\n",
    "    THINKING = \"thinking\"      # Processing with LLM\n",
    "    ACTING = \"acting\"          # Executing a tool\n",
    "    OBSERVING = \"observing\"    # Processing tool results\n",
    "    COMPLETED = \"completed\"    # Task finished\n",
    "    ERROR = \"error\"           # Something went wrong\n",
    "\n",
    "# Define what an action looks like\n",
    "@dataclass\n",
    "class AgentAction:\n",
    "    \"\"\"\n",
    "    Represents a single action the agent wants to take.\n",
    "    This is the bridge between thinking and doing.\n",
    "    \"\"\"\n",
    "    tool_name: str          # Which tool to use\n",
    "    tool_input: str         # What to pass to the tool\n",
    "    reasoning: str          # Why this action was chosen\n",
    "    confidence: float = 0.0 # How confident (0-1)\n",
    "\n",
    "# Configuration for our agent\n",
    "@dataclass\n",
    "class AgentConfig:\n",
    "    \"\"\"\n",
    "    Configuration parameters for agent behavior.\n",
    "    These knobs let us tune how the agent operates.\n",
    "    \"\"\"\n",
    "    name: str = \"Agent\"\n",
    "    model: str = \"qwen2.5:7b-instruct-q4_K_M\"  \n",
    "    max_iterations: int = 5          # Prevent infinite loops\n",
    "    verbose: bool = True             # Show reasoning process\n",
    "    temperature: float = 0.7         # LLM creativity (0=deterministic, 1=creative)\n",
    "    timeout_seconds: int = 30        # Max time per action\n",
    "    \n",
    "# The context/memory structure\n",
    "@dataclass \n",
    "class AgentContext:\n",
    "    \"\"\"\n",
    "    The agent's working memory - everything it needs to remember.\n",
    "    This is crucial for maintaining context across actions.\n",
    "    \"\"\"\n",
    "    goal: str                           # What we're trying to achieve\n",
    "    conversation_history: List[Dict] = field(default_factory=list)  # Past interactions\n",
    "    action_history: List[AgentAction] = field(default_factory=list) # What we've done\n",
    "    observations: List[str] = field(default_factory=list)           # What we've learned\n",
    "    current_plan: List[str] = field(default_factory=list)          # Steps to take\n",
    "    iteration: int = 0                  # How many cycles we've done\n",
    "    start_time: float = field(default_factory=time.time)           # When we started\n",
    "    \n",
    "    def add_to_history(self, role: str, content: str):\n",
    "        \"\"\"Add an interaction to conversation history\"\"\"\n",
    "        self.conversation_history.append({\n",
    "            \"role\": role,\n",
    "            \"content\": content,\n",
    "            \"timestamp\": time.time()\n",
    "        })\n",
    "    \n",
    "    def get_summary(self) -> str:\n",
    "        \"\"\"Get a summary of current context for the LLM\"\"\"\n",
    "        summary = f\"Goal: {self.goal}\\n\"\n",
    "        summary += f\"Iteration: {self.iteration}\\n\"\n",
    "        \n",
    "        if self.action_history:\n",
    "            summary += \"\\nPrevious actions:\\n\"\n",
    "            for action in self.action_history[-3:]:  # Last 3 actions\n",
    "                summary += f\"- {action.tool_name}: {action.reasoning}\\n\"\n",
    "        \n",
    "        if self.observations:\n",
    "            summary += \"\\nKey observations:\\n\"\n",
    "            for obs in self.observations[-3:]:  # Last 3 observations\n",
    "                summary += f\"- {obs}\\n\"\n",
    "                \n",
    "        return summary\n",
    "\n",
    "print(\"âœ… Core data structures defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ§  Part 2: LLM Integration with Ollama\n",
    "\n",
    "The LLM serves as the agent's \"brain\" - it:\n",
    "- **Reasons** about the current situation\n",
    "- **Decides** what action to take next\n",
    "- **Interprets** results from tools\n",
    "- **Plans** multi-step solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OllamaLLM:\n",
    "    \"\"\"\n",
    "    Our interface to Ollama - handles all LLM communication.\n",
    "    This is our agent's 'brain' that does the reasoning.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model: str = \"qwen2.5:7b-instruct-q4_K_M\", temperature: float = 0.7):\n",
    "        self.model = model\n",
    "        self.temperature = temperature\n",
    "        self.base_url = \"http://localhost:11434\"\n",
    "        \n",
    "    def generate(self, prompt: str, system: str = \"\") -> str:\n",
    "        \"\"\"\n",
    "        Generate a response from the LLM.\n",
    "        \n",
    "        Args:\n",
    "            prompt: The user prompt\n",
    "            system: System prompt to set behavior\n",
    "            \n",
    "        Returns:\n",
    "            The LLM's response text\n",
    "        \"\"\"\n",
    "        # Combine system and user prompts\n",
    "        full_prompt = f\"{system}\\n\\nUser: {prompt}\\n\\nAssistant:\" if system else prompt\n",
    "        \n",
    "        try:\n",
    "            response = requests.post(\n",
    "                f\"{self.base_url}/api/generate\",\n",
    "                json={\n",
    "                    \"model\": self.model,\n",
    "                    \"prompt\": full_prompt,\n",
    "                    \"temperature\": self.temperature,\n",
    "                    \"stream\": False\n",
    "                },\n",
    "                timeout=30\n",
    "            )\n",
    "            \n",
    "            if response.status_code == 200:\n",
    "                return response.json().get('response', '')\n",
    "            else:\n",
    "                raise Exception(f\"Ollama error: {response.status_code}\")\n",
    "                \n",
    "        except requests.exceptions.Timeout:\n",
    "            return \"Error: LLM request timed out. Try a shorter prompt.\"\n",
    "        except Exception as e:\n",
    "            return f\"Error: {str(e)}\"\n",
    "    \n",
    "    def generate_structured(self, prompt: str, system: str = \"\") -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Generate a structured response (JSON) from the LLM.\n",
    "        This is crucial for agent actions that need parsing.\n",
    "        \"\"\"\n",
    "        # Add JSON instruction to prompt\n",
    "        json_prompt = f\"{prompt}\\n\\nRespond ONLY with valid JSON, no other text.\"\n",
    "        \n",
    "        response = self.generate(json_prompt, system)\n",
    "        \n",
    "        # Try to parse JSON from response\n",
    "        try:\n",
    "            # Clean up response - LLMs sometimes add extra text\n",
    "            json_str = response.strip()\n",
    "            if \"```json\" in json_str:\n",
    "                json_str = json_str.split(\"```json\")[1].split(\"```\")[0]\n",
    "            elif \"```\" in json_str:\n",
    "                json_str = json_str.split(\"```\")[1].split(\"```\")[0]\n",
    "            \n",
    "            return json.loads(json_str)\n",
    "        except:\n",
    "            # Fallback for parsing errors\n",
    "            return {\n",
    "                \"error\": \"Failed to parse LLM response as JSON\",\n",
    "                \"raw_response\": response\n",
    "            }\n",
    "\n",
    "# Test the LLM integration\n",
    "llm = OllamaLLM(model=MODEL_NAME)\n",
    "\n",
    "print(\"ðŸ§ª Testing LLM integration...\")\n",
    "test_response = llm.generate(\n",
    "    \"Hello! Please respond with: 'LLM integration successful'\",\n",
    "    system=\"You are a helpful assistant.\"\n",
    ")\n",
    "print(f\"ðŸ“ LLM Response: {test_response[:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ”§ Part 3: Building Tools\n",
    "\n",
    "Tools extend the agent's capabilities beyond text generation. They're the agent's way of:\n",
    "- **Accessing** external information\n",
    "- **Performing** calculations\n",
    "- **Interacting** with systems\n",
    "- **Storing** and retrieving data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base tool interface\n",
    "class Tool:\n",
    "    \"\"\"\n",
    "    Base class for all tools. Every tool must:\n",
    "    1. Have a name and description\n",
    "    2. Implement the execute method\n",
    "    3. Handle errors gracefully\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, name: str, description: str):\n",
    "        self.name = name\n",
    "        self.description = description\n",
    "        \n",
    "    def execute(self, input_str: str) -> str:\n",
    "        \"\"\"Execute the tool with given input\"\"\"\n",
    "        raise NotImplementedError(\"Subclasses must implement execute\")\n",
    "        \n",
    "    def validate_input(self, input_str: str) -> Tuple[bool, str]:\n",
    "        \"\"\"Validate input before execution\"\"\"\n",
    "        if not input_str or not isinstance(input_str, str):\n",
    "            return False, \"Input must be a non-empty string\"\n",
    "        return True, \"Valid\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SearchTool(Tool):\n",
    "    \"\"\"\n",
    "    Simulated web search tool.\n",
    "    In production, this would call a real search API.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__(\n",
    "            name=\"search\",\n",
    "            description=\"Search for information on any topic. Input: search query\"\n",
    "        )\n",
    "        # Simulated knowledge base\n",
    "        self.knowledge_base = {\n",
    "            \"ai agents\": \"AI agents are autonomous systems that perceive, reason, and act to achieve goals.\",\n",
    "            \"react pattern\": \"ReAct combines reasoning and acting in a loop for better agent behavior.\",\n",
    "            \"llm\": \"Large Language Models are neural networks trained on vast text data.\",\n",
    "            \"python\": \"Python is a high-level programming language known for simplicity.\",\n",
    "            \"climate\": \"Climate change refers to long-term shifts in global temperatures.\"\n",
    "        }\n",
    "    \n",
    "    def execute(self, query: str) -> str:\n",
    "        valid, msg = self.validate_input(query)\n",
    "        if not valid:\n",
    "            return f\"Search error: {msg}\"\n",
    "            \n",
    "        query_lower = query.lower()\n",
    "        \n",
    "        # Find relevant results\n",
    "        results = []\n",
    "        for key, value in self.knowledge_base.items():\n",
    "            if any(word in query_lower for word in key.split()):\n",
    "                results.append(value)\n",
    "        \n",
    "        if results:\n",
    "            return f\"Search results for '{query}': \" + \" \".join(results[:2])\n",
    "        else:\n",
    "            return f\"No specific results found for '{query}'. Try different keywords.\"\n",
    "\n",
    "# Test the search tool\n",
    "search_tool = SearchTool()\n",
    "print(f\"Search test: {search_tool.execute('AI agents')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CalculatorTool(Tool):\n",
    "    \"\"\"\n",
    "    Safe calculator for mathematical expressions.\n",
    "    Uses eval() with strict input validation.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__(\n",
    "            name=\"calculator\", \n",
    "            description=\"Perform mathematical calculations. Input: mathematical expression\"\n",
    "        )\n",
    "        self.allowed_chars = set('0123456789+-*/()., ')\n",
    "        self.allowed_names = {'abs', 'round', 'min', 'max'}\n",
    "    \n",
    "    def execute(self, expression: str) -> str:\n",
    "        valid, msg = self.validate_input(expression)\n",
    "        if not valid:\n",
    "            return f\"Calculator error: {msg}\"\n",
    "        \n",
    "        # Security: validate expression characters\n",
    "        if not all(c in self.allowed_chars for c in expression):\n",
    "            return \"Error: Invalid characters in expression. Use only numbers and +-*/().\"\n",
    "        \n",
    "        try:\n",
    "            # Create safe namespace\n",
    "            safe_dict = {name: getattr(__builtins__, name) for name in self.allowed_names}\n",
    "            result = eval(expression, {\"__builtins__\": {}}, safe_dict)\n",
    "            return f\"Result: {result}\"\n",
    "        except ZeroDivisionError:\n",
    "            return \"Error: Division by zero\"\n",
    "        except Exception as e:\n",
    "            return f\"Error: Invalid expression - {str(e)}\"\n",
    "\n",
    "# Test calculator\n",
    "calc_tool = CalculatorTool()\n",
    "print(f\"Calculator test: {calc_tool.execute('(10 + 5) * 2')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MemoryTool(Tool):\n",
    "    \"\"\"\n",
    "    Simple key-value memory storage.\n",
    "    Allows agent to store and retrieve information.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__(\n",
    "            name=\"memory\",\n",
    "            description=\"Store or retrieve information. Input: 'store key=value' or 'get key'\"\n",
    "        )\n",
    "        self.storage = {}\n",
    "    \n",
    "    def execute(self, command: str) -> str:\n",
    "        valid, msg = self.validate_input(command)\n",
    "        if not valid:\n",
    "            return f\"Memory error: {msg}\"\n",
    "        \n",
    "        # First check if it starts with 'store ' or 'get '\n",
    "        if command.startswith('store '):\n",
    "            # Extract the key=value part\n",
    "            data = command[6:].strip()  # Remove 'store '\n",
    "            if '=' not in data:\n",
    "                return \"Error: Store format is 'store key=value'\"\n",
    "            key, value = data.split('=', 1)\n",
    "            self.storage[key.strip()] = value.strip()\n",
    "            return f\"Stored: {key.strip()} = {value.strip()}\"\n",
    "            \n",
    "        elif command.startswith('get '):\n",
    "            # Extract the key\n",
    "            key = command[4:].strip()  # Remove 'get '\n",
    "            if key in self.storage:\n",
    "                return f\"Retrieved: {key} = {self.storage[key]}\"\n",
    "            else:\n",
    "                return f\"Not found: {key}\"\n",
    "                \n",
    "        else:\n",
    "            return \"Error: Command must start with 'store ' or 'get '\"\n",
    "\n",
    "# Create tool collection\n",
    "tools = {\n",
    "    \"search\": SearchTool(),\n",
    "    \"calculator\": CalculatorTool(), \n",
    "    \"memory\": MemoryTool()\n",
    "}\n",
    "\n",
    "print(\"ðŸ”§ Tool collection created:\")\n",
    "for name, tool in tools.items():\n",
    "    print(f\"  - {name}: {tool.description}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ”„ Part 4: The ReAct Pattern\n",
    "\n",
    "### What is ReAct?\n",
    "\n",
    "ReAct (Reasoning + Acting) is a cognitive architecture that interleaves:\n",
    "- **Reasoning**: Thinking about what to do\n",
    "- **Acting**: Actually doing it\n",
    "- **Observing**: Learning from results\n",
    "\n",
    "### Why ReAct Works\n",
    "\n",
    "Traditional approaches separate thinking and acting. ReAct combines them:\n",
    "\n",
    "```\n",
    "Traditional:             ReAct:\n",
    "Think â†’ Think â†’ Act      Think â†’ Act â†’ Observe â†’ Think â†’ Act â†’ Observe\n",
    "        â†“                                                         â†“\n",
    "   Often wrong                                        Self-correcting\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReActAgent:\n",
    "    \"\"\"\n",
    "    Our main agent class implementing the ReAct pattern.\n",
    "    This is where everything comes together!\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config: AgentConfig):\n",
    "        self.config = config\n",
    "        self.llm = OllamaLLM(model=config.model, temperature=config.temperature)\n",
    "        self.tools = {}\n",
    "        self.state = AgentState.IDLE\n",
    "        self.context = None\n",
    "        \n",
    "    def add_tool(self, tool: Tool):\n",
    "        \"\"\"Register a tool with the agent\"\"\"\n",
    "        self.tools[tool.name] = tool\n",
    "        if self.config.verbose:\n",
    "            print(f\"âœ… Added tool: {tool.name}\")\n",
    "    \n",
    "    def _update_state(self, new_state: AgentState):\n",
    "        \"\"\"Update agent state with logging\"\"\"\n",
    "        if self.config.verbose:\n",
    "            print(f\"\\nðŸ”„ State: {self.state.value} â†’ {new_state.value}\")\n",
    "        self.state = new_state\n",
    "    \n",
    "    def _create_system_prompt(self) -> str:\n",
    "        \"\"\"Create the system prompt that defines agent behavior\"\"\"\n",
    "        tool_descriptions = \"\\n\".join([\n",
    "            f\"- {name}: {tool.description}\"\n",
    "            for name, tool in self.tools.items()\n",
    "        ])\n",
    "        \n",
    "        return f\"\"\"You are {self.config.name}, an autonomous AI agent using the ReAct pattern.\n",
    "\n",
    "You have access to these tools:\n",
    "{tool_descriptions}\n",
    "\n",
    "For each step, you must:\n",
    "1. THOUGHT: Analyze the current situation and plan your next action\n",
    "2. ACTION: Choose a tool and provide input\n",
    "3. Wait for OBSERVATION\n",
    "4. Repeat until the goal is achieved\n",
    "\n",
    "IMPORTANT: \n",
    "- Always start with a THOUGHT\n",
    "- Use tools to gather information or perform actions\n",
    "- Be concise and focused\n",
    "- Learn from observations to improve your approach\n",
    "\n",
    "Format your response as:\n",
    "THOUGHT: [your reasoning]\n",
    "ACTION: [tool_name] [input]\n",
    "\"\"\"\n",
    "    \n",
    "    def _parse_llm_response(self, response: str) -> Optional[AgentAction]:\n",
    "        \"\"\"Parse LLM response to extract action\"\"\"\n",
    "        lines = response.strip().split('\\n')\n",
    "        \n",
    "        thought = \"\"\n",
    "        action_line = \"\"\n",
    "        \n",
    "        for line in lines:\n",
    "            if line.strip().startswith(\"THOUGHT:\"):\n",
    "                thought = line.replace(\"THOUGHT:\", \"\").strip()\n",
    "            elif line.strip().startswith(\"ACTION:\"):\n",
    "                action_line = line.replace(\"ACTION:\", \"\").strip()\n",
    "        \n",
    "        if not action_line:\n",
    "            return None\n",
    "            \n",
    "        # Parse action line\n",
    "        parts = action_line.split(maxsplit=1)\n",
    "        if len(parts) < 2:\n",
    "            return None\n",
    "            \n",
    "        tool_name = parts[0]\n",
    "        tool_input = parts[1] if len(parts) > 1 else \"\"\n",
    "        \n",
    "        return AgentAction(\n",
    "            tool_name=tool_name,\n",
    "            tool_input=tool_input,\n",
    "            reasoning=thought\n",
    "        )\n",
    "    \n",
    "    def _execute_action(self, action: AgentAction) -> str:\n",
    "        \"\"\"Execute an action using the appropriate tool\"\"\"\n",
    "        if action.tool_name not in self.tools:\n",
    "            return f\"Error: Unknown tool '{action.tool_name}'\"\n",
    "            \n",
    "        tool = self.tools[action.tool_name]\n",
    "        \n",
    "        try:\n",
    "            result = tool.execute(action.tool_input)\n",
    "            return result\n",
    "        except Exception as e:\n",
    "            return f\"Error executing {action.tool_name}: {str(e)}\"\n",
    "    \n",
    "    def think(self, context: AgentContext) -> Optional[AgentAction]:\n",
    "        \"\"\"Generate next action using LLM reasoning\"\"\"\n",
    "        self._update_state(AgentState.THINKING)\n",
    "        \n",
    "        # Build prompt with context\n",
    "        prompt = f\"\"\"Current context:\n",
    "{context.get_summary()}\n",
    "\n",
    "What should I do next to achieve the goal?\n",
    "\"\"\"\n",
    "        \n",
    "        # Get LLM response\n",
    "        response = self.llm.generate(prompt, self._create_system_prompt())\n",
    "        \n",
    "        if self.config.verbose:\n",
    "            print(f\"\\nðŸ’­ LLM Response:\\n{response}\")\n",
    "        \n",
    "        # Parse action from response\n",
    "        action = self._parse_llm_response(response)\n",
    "        \n",
    "        if action:\n",
    "            context.add_to_history(\"assistant\", response)\n",
    "            context.action_history.append(action)\n",
    "            \n",
    "        return action\n",
    "    \n",
    "    def act(self, action: AgentAction) -> str:\n",
    "        \"\"\"Execute the chosen action\"\"\"\n",
    "        self._update_state(AgentState.ACTING)\n",
    "        \n",
    "        if self.config.verbose:\n",
    "            print(f\"\\nðŸ”§ Executing: {action.tool_name} with input: {action.tool_input}\")\n",
    "        \n",
    "        result = self._execute_action(action)\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def observe(self, observation: str, context: AgentContext):\n",
    "        \"\"\"Process the observation from action\"\"\"\n",
    "        self._update_state(AgentState.OBSERVING)\n",
    "        \n",
    "        if self.config.verbose:\n",
    "            print(f\"\\nðŸ‘ï¸ Observation: {observation}\")\n",
    "        \n",
    "        context.observations.append(observation)\n",
    "        context.add_to_history(\"observation\", observation)\n",
    "    \n",
    "    def run(self, goal: str) -> str:\n",
    "        \"\"\"Run the agent to achieve a goal\"\"\"\n",
    "        print(f\"\\nðŸŽ¯ Starting agent with goal: {goal}\")\n",
    "        \n",
    "        # Initialize context\n",
    "        context = AgentContext(goal=goal)\n",
    "        self.context = context\n",
    "        \n",
    "        # Main agent loop\n",
    "        while context.iteration < self.config.max_iterations:\n",
    "            context.iteration += 1\n",
    "            \n",
    "            if self.config.verbose:\n",
    "                print(f\"\\n{'='*50}\")\n",
    "                print(f\"Iteration {context.iteration}/{self.config.max_iterations}\")\n",
    "                print(f\"{'='*50}\")\n",
    "            \n",
    "            # Think\n",
    "            action = self.think(context)\n",
    "            if not action:\n",
    "                print(\"\\nâŒ Could not determine next action\")\n",
    "                break\n",
    "            \n",
    "            # Act\n",
    "            result = self.act(action)\n",
    "            \n",
    "            # Observe\n",
    "            self.observe(result, context)\n",
    "            \n",
    "            # Check if goal achieved (simple heuristic)\n",
    "            if \"error\" not in result.lower() and context.iteration > 1:\n",
    "                # Ask LLM if goal is achieved\n",
    "                check_prompt = f\"\"\"Based on the context and observations, has the goal been achieved?\n",
    "Goal: {goal}\n",
    "Latest observation: {result}\n",
    "\n",
    "Answer with just YES or NO.\"\"\"\n",
    "                \n",
    "                check_response = self.llm.generate(check_prompt).strip().upper()\n",
    "                if \"YES\" in check_response:\n",
    "                    self._update_state(AgentState.COMPLETED)\n",
    "                    print(\"\\nâœ… Goal achieved!\")\n",
    "                    break\n",
    "        \n",
    "        # Prepare final summary\n",
    "        if context.iteration >= self.config.max_iterations:\n",
    "            print(\"\\nâ° Reached maximum iterations\")\n",
    "        \n",
    "        return self._generate_summary(context)\n",
    "    \n",
    "    def _generate_summary(self, context: AgentContext) -> str:\n",
    "        \"\"\"Generate a summary of the agent's work\"\"\"\n",
    "        summary = f\"\\nðŸ“Š Agent Summary:\\n\"\n",
    "        summary += f\"Goal: {context.goal}\\n\"\n",
    "        summary += f\"Iterations: {context.iteration}\\n\"\n",
    "        summary += f\"Actions taken: {len(context.action_history)}\\n\"\n",
    "        summary += f\"Final state: {self.state.value}\\n\"\n",
    "        \n",
    "        if context.observations:\n",
    "            summary += f\"\\nKey findings:\\n\"\n",
    "            for obs in context.observations[-3:]:\n",
    "                summary += f\"- {obs}\\n\"\n",
    "        \n",
    "        return summary\n",
    "\n",
    "# Create and configure agent\n",
    "print(\"\\nðŸ¤– Creating ReAct agent...\")\n",
    "agent_config = AgentConfig(\n",
    "    name=\"ResearchBot\",\n",
    "    model=MODEL_NAME,\n",
    "    max_iterations=5,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "agent = ReActAgent(agent_config)\n",
    "\n",
    "# Add tools\n",
    "for tool in tools.values():\n",
    "    agent.add_tool(tool)\n",
    "\n",
    "print(\"\\nâœ… Agent ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸš€ Part 5: Agent in Action\n",
    "\n",
    "Let's see our agent solve real problems. Watch how it:\n",
    "1. Breaks down the goal\n",
    "2. Chooses appropriate tools\n",
    "3. Learns from observations\n",
    "4. Achieves the objective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo 1: Simple calculation task\n",
    "print(\"ðŸ“ Demo 1: Simple Calculation\")\n",
    "print(\"Task: Calculate the area of a rectangle with width 15 and height 23\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "result = agent.run(\"Calculate the area of a rectangle with width 15 and height 23\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo 2: Multi-step research task\n",
    "print(\"\\nðŸ“ Demo 2: Research Task\")\n",
    "print(\"Task: Research AI agents and store key findings\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "result = agent.run(\"Research what AI agents are and store the key findings in memory\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸŽ¯ Hands-On Exercise: Build Your Own Tool\n",
    "\n",
    "Create a custom tool that the agent can use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise: Build a Weather Tool\n",
    "class WeatherTool(Tool):\n",
    "    \"\"\"\n",
    "    TODO: Implement a weather tool that:\n",
    "    1. Takes a city name as input\n",
    "    2. Returns mock weather data\n",
    "    3. Handles invalid cities gracefully\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__(\n",
    "            name=\"weather\",\n",
    "            description=\"Get weather for a city. Input: city name\"\n",
    "        )\n",
    "        # TODO: Add mock weather data\n",
    "        self.weather_data = {\n",
    "            \"london\": \"Cloudy, 15Â°C, 70% humidity\",\n",
    "            \"paris\": \"Sunny, 22Â°C, 50% humidity\",\n",
    "            \"tokyo\": \"Rainy, 18Â°C, 80% humidity\",\n",
    "            \"new york\": \"Clear, 20Â°C, 60% humidity\"\n",
    "        }\n",
    "    \n",
    "    def execute(self, city: str) -> str:\n",
    "        # TODO: Implement weather lookup\n",
    "        valid, msg = self.validate_input(city)\n",
    "        if not valid:\n",
    "            return f\"Weather error: {msg}\"\n",
    "            \n",
    "        city_lower = city.lower().strip()\n",
    "        \n",
    "        if city_lower in self.weather_data:\n",
    "            return f\"Weather in {city}: {self.weather_data[city_lower]}\"\n",
    "        else:\n",
    "            return f\"Weather data not available for {city}\"\n",
    "\n",
    "# Test your implementation\n",
    "weather_tool = WeatherTool()\n",
    "print(weather_tool.execute(\"London\"))\n",
    "print(weather_tool.execute(\"Mumbai\"))\n",
    "\n",
    "# Add to agent\n",
    "agent.add_tool(weather_tool)\n",
    "\n",
    "# Test with agent\n",
    "result = agent.run(\"What's the weather in Paris and Tokyo?\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“Š Key Takeaways\n",
    "\n",
    "### What You've Learned:\n",
    "\n",
    "1. **Agent Architecture**: State, Context, Actions, and Control Flow\n",
    "2. **ReAct Pattern**: Think â†’ Act â†’ Observe loop enables self-correction\n",
    "3. **Tool Design**: Extend capabilities with clean interfaces\n",
    "4. **LLM Integration**: Structure prompts for reliable agent behavior\n",
    "5. **Error Handling**: Make agents robust with validation and fallbacks\n",
    "\n",
    "### ReAct Performance:\n",
    "\n",
    "From the research:\n",
    "- **34% improvement** on ALFWorld tasks\n",
    "- **10% improvement** on WebShop navigation\n",
    "- Key: Interleaving reasoning with action\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "In Module 1.3, we'll explore:\n",
    "- **ReWOO**: 64% token reduction vs ReAct\n",
    "- Side-by-side performance comparison\n",
    "- When to use each pattern\n",
    "\n",
    "Ready to optimize? Let's go! ðŸš€"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}